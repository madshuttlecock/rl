{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14pt\">MIPT, Applied ML, Autumn 2018</span>\n",
    "\n",
    "<span style=\"font-size: 16pt\"> HW #5: Approximate RL homework\n",
    "\n",
    "<span style=\"color:red; font-size: 14pt;\"> Дедлайн 5.12.2018 12:00 </span>\n",
    "\n",
    "<span style=\"color:blue; font-size: 12pt\">Valentin Malykh </span>,\n",
    "<span style=\"color:blue; font-size: 12pt; font-family: 'Verdana'\">val@maly.hk</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Оформление дз**: \n",
    "- Выполненное задание требуется отправлять через <a href='https://goo.gl/forms/XPSIbwp7wPxB4SsI3'>форму </a>\n",
    "\n",
    "- Выполненное дз прикрепляйте в формате файла ``<фамилия>_<группа>_task<номер>.ipynb``, например: ``ivanov_594_task4.ipynb`` \n",
    "\n",
    "**Вопросы**:\n",
    "- Вопросы присылайте в канал в телеграмме ``[Fall 2018]ML Seminars``\n",
    "\n",
    "--------\n",
    "- **PS1**: Будьте внимательны при заполнении формы, когда отправляете ДЗ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Практическое задание (100%)</h1>\n",
    "Описание находится по ссылке: https://gist.github.com/madrugado/bbaedcb614755d47345c3c69b1b41169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Я реализовала алгоритм Proximal-Policy-Optimization. Я руководствовалась вот этим https://junhongxu.github.io/Proximal-Policy-Optimization/. Автор этого репозитория утверждает, что набирает реворд 275. Однако, вглядевшись в код, я заметила, что он считает реворд, как сумму за 2048 итераций, а не за игру. Если считать так же, как он, то моя реализация набирает реворд 550, а средний реворд за игру 260.**\n",
    "\n",
    "Суммарно я потратила на это задание около недели, при этом почти с утра до ночи, потому что DDPG не учился, а в PPO я выискивала неочевидные баги(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA:  True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"USE_CUDA: \", use_cuda)\n",
    "Tensor = FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    " EPS = 0.003\n",
    "class Approx(nn.Module):\n",
    "    def __init__(self, state_size, hid_size=64):\n",
    "        print(hid_size)\n",
    "        super(Approx, self).__init__()\n",
    "        self.full1 = nn.Linear(state_size, hid_size)\n",
    "        #self.full1.weight.data = fanin_init(self.full1.weight.data.size())\n",
    "        print(self.full1.weight.data.shape)\n",
    "        self.full2 = nn.Linear(hid_size, 2 * hid_size)\n",
    "        print(self.full2.weight.data.shape)\n",
    "        self.full5 = nn.Linear(2 * hid_size, 1)\n",
    "        \n",
    "        print(self.full5.weight.data.shape)\n",
    "        self.full5.weight.data.mul(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"S\", x.shape, y.shape)\n",
    "        x = F.tanh(self.full1(x))\n",
    "        x = F.tanh(self.full2(x))\n",
    "\n",
    "        x = self.full5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_density(x, mean, log_std, std):\n",
    "    variance = std.pow(2)\n",
    "    log_density = -(x - mean).pow(2) / (2 * variance) - 0.5 *\\\n",
    "        np.log(2 * np.pi) - log_std\n",
    "    log_density = log_density.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    return log_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hid_size=64):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.act_fc1 = nn.Linear(state_size, 64)\n",
    "        self.act_fc2 = nn.Linear(64, 128)\n",
    "        self.mu = nn.Linear(128, action_size)\n",
    "        self.mu.weight.data.mul_(0.1)\n",
    "        # torch.log(std)\n",
    "        print(self.act_fc1.weight.data.shape)\n",
    "        print(self.act_fc2.weight.data.shape)\n",
    "        print(self.mu.weight.data.shape)\n",
    "        self.logstd = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        act = self.act_fc1(x)\n",
    "        act = F.tanh(act)\n",
    "        act = self.act_fc2(act)\n",
    "        act = F.tanh(act)\n",
    "        mean = self.mu(act)  # N, num_actions\n",
    "        logstd = self.logstd.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "        action = torch.normal(mean, std)\n",
    "        \n",
    "        logprob = log_normal_density(action, mean, std=std, log_std=logstd)\n",
    "        \n",
    "        return action, logprob, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actions(policy, approx, x, action):\n",
    "    v = approx.forward(x)\n",
    "    _, _, mean = policy.forward(x)\n",
    "    #print(\"E\", policy.logstd)\n",
    "    logstd = policy.logstd.expand_as(mean)\n",
    "    #print(\"U\", logstd)\n",
    "    std = torch.exp(logstd)\n",
    "    # evaluate\n",
    "    logprob = log_normal_density(action, mean, log_std=logstd, std=std)\n",
    "    dist_entropy = 0.5 + 0.5 * math.log(2 * math.pi) + logstd\n",
    "    #print(\"E\", dist_entropy)\n",
    "    dist_entropy = dist_entropy.sum(-1).mean()\n",
    "    #print(\"W\", dist_entropy.shape)\n",
    "    return v, logprob, dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0], env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, dones, last_value, gamma=0.99):\n",
    "    returns = np.zeros(len(rewards) + 1)\n",
    "    returns[-1] = last_value\n",
    "   \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = gamma * returns[i+1] * (1 - dones[i]) + rewards[i]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic(object):\n",
    "    def __init__(self, env, \n",
    "                    gamma=0.99,\n",
    "                    lr=2e-4,\n",
    "                    test_episodes=30,\n",
    "                    num_iter=7,\n",
    "                    batch_size=64,\n",
    "                    coef_entropy=1e-4, \n",
    "                    clip_value=0.2):\n",
    "        super(Actor_Critic, self).__init__()\n",
    "        self.num_iter = num_iter\n",
    "        self.coef_entropy=coef_entropy\n",
    "        self.clip_value=clip_value\n",
    "        self.env = env\n",
    "        self.state_dim, self.action_dim, self.action_lim = (env.observation_space.shape[0], env.action_space.shape[0],\n",
    "                                                                            env.action_space.high[0])\n",
    "        \n",
    "        self.actor = Policy(self.state_dim, self.action_dim)\n",
    "        \n",
    "\n",
    "        self.critic = Approx(self.state_dim)\n",
    "        \n",
    "        self.optimizer = optim.Adam(list(self.actor.parameters()) + list(self.critic.parameters()),lr=lr)\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            self.actor.cuda()\n",
    "            self.critic.cuda()\n",
    "            \n",
    "        \n",
    "        self.test_episodes = test_episodes\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.train_rewards = []\n",
    "        self.test_rewards = []\n",
    "        self.train_steps = []\n",
    "        self.test_steps = []\n",
    "        self.losses_w = []\n",
    "        self.losses_theta = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "        self.cnt=0\n",
    "        \n",
    "\n",
    "\n",
    "    def play_one_step(self, s, it=0):\n",
    "        \"One step\"\n",
    "\n",
    "        action, logprob, mean = self.actor.forward(Variable(Tensor(s[np.newaxis])))\n",
    "        action, logprob = action.data.cpu().numpy()[0], logprob.data.cpu().numpy()[0]\n",
    "        value = (self.critic.forward(Variable(Tensor(s[np.newaxis])))).data.cpu().numpy()[0]\n",
    "        self.observations.append(np.array(s))\n",
    "        \n",
    "        self.logprobs.append(logprob)\n",
    "        self.values.append(value[0])\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        s1, r, is_terminal, _ = self.env.step(action)\n",
    "        self.rewards.append(r)\n",
    "        self.dones.append(is_terminal)\n",
    "        \n",
    "        s1 = np.float32(s1)\n",
    "\n",
    "        return s1, r, is_terminal\n",
    "    \n",
    "    def play_game(self):\n",
    "        \n",
    "        '''Играет фиксированное количество итераций. Может быть несколько игр'''\n",
    "        t = time.clock()\n",
    "\n",
    "        state = np.float32(self.env.reset())\n",
    "        \n",
    "        #self.history.clear()\n",
    "        MAX_STEP = 2100\n",
    "        frames = []\n",
    "        it = 1\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.logprobs = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "        self.actions= []\n",
    "        game = 0\n",
    "        rewards = []\n",
    "        sum_rewards = []\n",
    "        steps = []\n",
    "        while True:\n",
    "            state1, reward, is_terminal = self.play_one_step(state, it)\n",
    "            rewards += [reward]\n",
    "            if is_terminal:\n",
    "                # начало новой игры\n",
    "                state = env.reset()\n",
    "                steps.append(game)\n",
    "                sum_rewards.append(sum(rewards))\n",
    "                rewards = []\n",
    "                game = 0\n",
    "            #rewards.append(reward)\n",
    "            it +=1\n",
    "            game += 1\n",
    "            state = state1\n",
    "\n",
    "            if it == MAX_STEP:\n",
    "                break\n",
    "     \n",
    "        last_value = 0\n",
    "        if not is_terminal:\n",
    "            obs = Tensor(state1[np.newaxis])\n",
    "            value = self.critic(obs)\n",
    "            last_value = value.data[0][0]\n",
    "        returns = calculate_returns(self.rewards, self.dones, last_value)[:-1]\n",
    "        \n",
    "        \n",
    "        ii = np.arange(len(self.actions)).reshape(-1, 1)\n",
    "        self.observations = np.array(self.observations)\n",
    "        self.actions = np.array(self.actions)\n",
    "        self.logprobs = np.array(self.logprobs).reshape(-1, 1)\n",
    "        self.rewards = np.array(self.rewards).reshape(-1, 1)\n",
    "        self.values = np.array(self.values).reshape(-1, 1)\n",
    "        returns = np.array(returns).reshape(-1, 1)\n",
    "        advantages = returns - self.values\n",
    "        #в блоге утверждается, что это ускоряет обучение\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        tt = time.clock()\n",
    "        \n",
    "        for e in range(self.num_iter):\n",
    "            # несколько раз семплируем батчи и обучаемся\n",
    "            sampler = BatchSampler(SubsetRandomSampler(list(range(advantages.shape[0]))), \n",
    "                                   batch_size=self.batch_size,\n",
    "                                   drop_last=False)\n",
    "            for i, index in enumerate(sampler):\n",
    "                sampled = [0 for i in range(8)]\n",
    "                sampled[0] = Tensor(self.observations[index])\n",
    "\n",
    "                sampled[1] = Tensor(self.actions[index])\n",
    "                sampled[2] = Tensor(self.logprobs[index])\n",
    "                sampled[3] = Tensor(returns[index])\n",
    "                sampled[4] = Tensor(advantages[index])\n",
    "                sampled[5] = Tensor(ii[index])\n",
    "                sampled[6] = Tensor(self.values[index])\n",
    "                sampled[7] = Tensor(self.rewards[index])\n",
    "                \n",
    "                self.optimize(sampled)\n",
    "        tt = time.clock()\n",
    "        \n",
    "        return self.rewards, [it]\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch):\n",
    "        '''Optimizes loss on mini_batch'''\n",
    "        i = mini_batch[5]\n",
    "        \n",
    "        s = mini_batch[0]\n",
    "        a = mini_batch[1]\n",
    "        logprobs = mini_batch[2]\n",
    "        v = mini_batch[6]\n",
    "        r = mini_batch[7]\n",
    "        returns = mini_batch[3]\n",
    "        advantages = mini_batch[4]\n",
    "\n",
    "        \n",
    "        new_value, new_logprob, dist_entropy = evaluate_actions(self.actor, self.critic, s, a)\n",
    "        sampled_logprobs = logprobs.view(-1, 1)\n",
    "\n",
    "        ratio = torch.exp(new_logprob - sampled_logprobs)\n",
    "        \n",
    "        sampled_advs = advantages#.view(-1, 1)\n",
    "\n",
    "        surrogate1 = ratio * sampled_advs\n",
    "        surrogate2 = torch.clamp(ratio, 1 - self.clip_value, 1 + self.clip_value) * sampled_advs\n",
    "\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        \n",
    "        sampled_returns = returns\n",
    "        \n",
    "        value_loss = F.mse_loss(new_value, sampled_returns)\n",
    "\n",
    "        loss =  value_loss - self.coef_entropy * dist_entropy + policy_loss\n",
    "        \n",
    "        t = time.clock() \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        tt = time.clock() \n",
    "   \n",
    "        \n",
    "    \n",
    "\n",
    "                \n",
    "    def train(self, num_episodes, save_ep=1000, path='.'):\n",
    "        \n",
    "        self.save_ep = save_ep\n",
    "        self.q = time.clock()\n",
    "        for it in tqdm_notebook(range(num_episodes)):\n",
    "            \n",
    "            rewards, steps = self.play_game()\n",
    "        \n",
    "            self.train_rewards += [sum(rewards)]\n",
    "            self.train_steps += steps\n",
    "                \n",
    "            if (it+1) % 30 == 0:\n",
    "                print(\"It: \", it+1, \"reward: \", sum(rewards), \"steps: \", steps)\n",
    "                print(\"Mean reward: \", np.mean(np.array(self.train_rewards[-30:])))\n",
    "                print(\"Max reward: \", np.max(np.array(self.train_rewards[-30:])))\n",
    "            if (it) % (self.save_ep) == 0:\n",
    "                torch.save(self.actor.state_dict(), \"actor_{}.pkl\".format(it+1))\n",
    "                torch.save(self.critic.state_dict(), \"critic_{}.pkl\".format(it+1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 24])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([4, 128])\n",
      "64\n",
      "torch.Size([64, 24])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "\n",
    "\n",
    "\n",
    "agent = Actor_Critic(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcb4f653b9b4134a2c2455fa6f4a980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  30 reward:  [-97.07760611] steps:  [2100]\n",
      "Mean reward:  -408.05289308050266\n",
      "Max reward:  -97.07760610838051\n",
      "It:  60 reward:  [-269.43642273] steps:  [2100]\n",
      "Mean reward:  -157.1034998519414\n",
      "Max reward:  -55.763304690922844\n",
      "It:  90 reward:  [-43.04090008] steps:  [2100]\n",
      "Mean reward:  -100.66386217270112\n",
      "Max reward:  -16.65478804527251\n",
      "It:  120 reward:  [28.08066106] steps:  [2100]\n",
      "Mean reward:  -26.252632754756608\n",
      "Max reward:  28.08066106485138\n",
      "It:  150 reward:  [-17.80448907] steps:  [2100]\n",
      "Mean reward:  27.83649370629946\n",
      "Max reward:  109.48918313607584\n",
      "It:  180 reward:  [-111.21672501] steps:  [2100]\n",
      "Mean reward:  50.33197050869353\n",
      "Max reward:  165.6798149249069\n",
      "It:  210 reward:  [98.70462825] steps:  [2100]\n",
      "Mean reward:  14.049158603436588\n",
      "Max reward:  150.39524475822026\n",
      "It:  240 reward:  [161.53233768] steps:  [2100]\n",
      "Mean reward:  93.75829457246383\n",
      "Max reward:  174.48607814324723\n",
      "It:  270 reward:  [-52.57596218] steps:  [2100]\n",
      "Mean reward:  113.87364389986854\n",
      "Max reward:  210.30672282653185\n",
      "It:  300 reward:  [217.86064851] steps:  [2100]\n",
      "Mean reward:  150.74838236019804\n",
      "Max reward:  233.9743907103849\n",
      "It:  330 reward:  [-17.59280586] steps:  [2100]\n",
      "Mean reward:  86.42833127696044\n",
      "Max reward:  249.95709761282987\n",
      "It:  360 reward:  [205.49331811] steps:  [2100]\n",
      "Mean reward:  120.69359179680977\n",
      "Max reward:  239.9208476481232\n",
      "It:  390 reward:  [260.19576556] steps:  [2100]\n",
      "Mean reward:  190.54105089013657\n",
      "Max reward:  260.5779870401971\n",
      "It:  420 reward:  [251.4262756] steps:  [2100]\n",
      "Mean reward:  163.7362090112517\n",
      "Max reward:  275.4904029929114\n",
      "It:  450 reward:  [253.24776856] steps:  [2100]\n",
      "Mean reward:  203.98037035349537\n",
      "Max reward:  266.7816628288577\n",
      "It:  480 reward:  [153.87757813] steps:  [2100]\n",
      "Mean reward:  177.9482048727883\n",
      "Max reward:  301.17463136165765\n",
      "It:  510 reward:  [304.96601139] steps:  [2100]\n",
      "Mean reward:  275.6262593566152\n",
      "Max reward:  312.0533496530675\n",
      "It:  540 reward:  [326.35440345] steps:  [2100]\n",
      "Mean reward:  293.13648669997195\n",
      "Max reward:  340.7734751454266\n",
      "It:  570 reward:  [310.93260506] steps:  [2100]\n",
      "Mean reward:  293.0022251498905\n",
      "Max reward:  335.95977079639675\n",
      "It:  600 reward:  [198.13352282] steps:  [2100]\n",
      "Mean reward:  197.17357680619546\n",
      "Max reward:  351.4704848401742\n",
      "It:  630 reward:  [220.42992997] steps:  [2100]\n",
      "Mean reward:  280.0155836819793\n",
      "Max reward:  356.8674368429347\n",
      "It:  660 reward:  [351.43715365] steps:  [2100]\n",
      "Mean reward:  287.97042820518834\n",
      "Max reward:  367.2109797175209\n",
      "It:  690 reward:  [324.12972553] steps:  [2100]\n",
      "Mean reward:  312.72918353625766\n",
      "Max reward:  389.47147262716624\n",
      "It:  720 reward:  [124.76815056] steps:  [2100]\n",
      "Mean reward:  318.15891283296173\n",
      "Max reward:  369.8396869514544\n",
      "It:  750 reward:  [351.94846017] steps:  [2100]\n",
      "Mean reward:  322.75591927874865\n",
      "Max reward:  378.10124829187225\n",
      "It:  780 reward:  [346.76834757] steps:  [2100]\n",
      "Mean reward:  295.594225139512\n",
      "Max reward:  382.993954945966\n",
      "It:  810 reward:  [347.00260833] steps:  [2100]\n",
      "Mean reward:  304.09892510630914\n",
      "Max reward:  376.4662240726292\n",
      "It:  840 reward:  [327.81512869] steps:  [2100]\n",
      "Mean reward:  287.22872336946153\n",
      "Max reward:  371.2693901708582\n",
      "It:  870 reward:  [338.76444135] steps:  [2100]\n",
      "Mean reward:  270.4647139779321\n",
      "Max reward:  363.39588420963685\n",
      "It:  900 reward:  [40.72962724] steps:  [2100]\n",
      "Mean reward:  291.62804116777255\n",
      "Max reward:  390.3694777390841\n",
      "It:  930 reward:  [301.01625782] steps:  [2100]\n",
      "Mean reward:  43.172400958190536\n",
      "Max reward:  412.38345581428575\n",
      "It:  960 reward:  [369.13470315] steps:  [2100]\n",
      "Mean reward:  320.95644809355787\n",
      "Max reward:  398.10464908763237\n",
      "It:  990 reward:  [210.20726468] steps:  [2100]\n",
      "Mean reward:  318.87455366253806\n",
      "Max reward:  381.570825942553\n",
      "It:  1020 reward:  [362.38554946] steps:  [2100]\n",
      "Mean reward:  295.738241139098\n",
      "Max reward:  362.38554946406174\n",
      "It:  1050 reward:  [396.82120074] steps:  [2100]\n",
      "Mean reward:  296.55551458878193\n",
      "Max reward:  396.82120073991535\n",
      "It:  1080 reward:  [350.72906295] steps:  [2100]\n",
      "Mean reward:  331.401309789822\n",
      "Max reward:  403.55270066738944\n",
      "It:  1110 reward:  [394.94665794] steps:  [2100]\n",
      "Mean reward:  341.02426144657926\n",
      "Max reward:  407.78777516033574\n",
      "It:  1140 reward:  [383.16848315] steps:  [2100]\n",
      "Mean reward:  267.300665734188\n",
      "Max reward:  410.5608280287549\n",
      "It:  1170 reward:  [281.25950072] steps:  [2100]\n",
      "Mean reward:  307.19041165746046\n",
      "Max reward:  409.6845255065994\n",
      "It:  1200 reward:  [9.92311893] steps:  [2100]\n",
      "Mean reward:  247.02898091121762\n",
      "Max reward:  423.9759843257452\n",
      "It:  1230 reward:  [398.04541557] steps:  [2100]\n",
      "Mean reward:  330.1933234141589\n",
      "Max reward:  425.3114170831276\n",
      "It:  1260 reward:  [296.28145121] steps:  [2100]\n",
      "Mean reward:  377.1678155945769\n",
      "Max reward:  418.3210379986744\n",
      "It:  1290 reward:  [286.09764506] steps:  [2100]\n",
      "Mean reward:  290.9534581934266\n",
      "Max reward:  427.39950563978465\n",
      "It:  1320 reward:  [-489.96245087] steps:  [2100]\n",
      "Mean reward:  274.2173138661458\n",
      "Max reward:  410.9876431787478\n",
      "It:  1350 reward:  [389.13544763] steps:  [2100]\n",
      "Mean reward:  272.67152794866263\n",
      "Max reward:  401.2610707072862\n",
      "It:  1380 reward:  [365.54041449] steps:  [2100]\n",
      "Mean reward:  329.1723778167451\n",
      "Max reward:  405.6894442931415\n",
      "It:  1410 reward:  [384.57122879] steps:  [2100]\n",
      "Mean reward:  383.85938992273896\n",
      "Max reward:  419.484602363224\n",
      "It:  1440 reward:  [384.80826682] steps:  [2100]\n",
      "Mean reward:  259.22690724918573\n",
      "Max reward:  418.7761094024796\n",
      "It:  1470 reward:  [372.26669131] steps:  [2100]\n",
      "Mean reward:  234.891752495564\n",
      "Max reward:  406.26666623286104\n",
      "It:  1500 reward:  [394.60420818] steps:  [2100]\n",
      "Mean reward:  351.74725396586416\n",
      "Max reward:  422.9412942655657\n",
      "It:  1530 reward:  [156.22885986] steps:  [2100]\n",
      "Mean reward:  201.13712414879708\n",
      "Max reward:  426.45431245147154\n",
      "It:  1560 reward:  [242.26297033] steps:  [2100]\n",
      "Mean reward:  233.18204513512072\n",
      "Max reward:  408.60735869052587\n",
      "It:  1590 reward:  [362.55804093] steps:  [2100]\n",
      "Mean reward:  269.17023636702066\n",
      "Max reward:  406.4127324498161\n",
      "It:  1620 reward:  [251.28956091] steps:  [2100]\n",
      "Mean reward:  307.9335902394612\n",
      "Max reward:  386.9668001678848\n",
      "It:  1650 reward:  [335.59028362] steps:  [2100]\n",
      "Mean reward:  281.70272597219815\n",
      "Max reward:  400.8095391509308\n",
      "It:  1680 reward:  [372.42738536] steps:  [2100]\n",
      "Mean reward:  210.4184044131575\n",
      "Max reward:  432.90559013523716\n",
      "It:  1710 reward:  [272.10575697] steps:  [2100]\n",
      "Mean reward:  261.3048272090837\n",
      "Max reward:  400.209319111166\n",
      "It:  1740 reward:  [374.81397164] steps:  [2100]\n",
      "Mean reward:  285.42688859204185\n",
      "Max reward:  420.2417730024199\n",
      "It:  1770 reward:  [364.69019921] steps:  [2100]\n",
      "Mean reward:  299.25899961483043\n",
      "Max reward:  402.11997873647124\n",
      "It:  1800 reward:  [378.24846849] steps:  [2100]\n",
      "Mean reward:  301.2455760917747\n",
      "Max reward:  404.92079035462643\n",
      "It:  1860 reward:  [380.0643673] steps:  [2100]\n",
      "Mean reward:  345.36936713792295\n",
      "Max reward:  424.87062328961565\n",
      "It:  1890 reward:  [234.40993105] steps:  [2100]\n",
      "Mean reward:  252.96398717304498\n",
      "Max reward:  427.74231783200617\n",
      "It:  1920 reward:  [259.38945474] steps:  [2100]\n",
      "Mean reward:  285.81062746273113\n",
      "Max reward:  414.8411268671772\n",
      "It:  1950 reward:  [424.14758892] steps:  [2100]\n",
      "Mean reward:  272.0377584215088\n",
      "Max reward:  424.14758892101844\n",
      "It:  1980 reward:  [76.23963479] steps:  [2100]\n",
      "Mean reward:  154.39252067012077\n",
      "Max reward:  433.896063761236\n",
      "It:  2010 reward:  [265.95313394] steps:  [2100]\n",
      "Mean reward:  310.64276852998086\n",
      "Max reward:  428.4442281770602\n",
      "It:  2040 reward:  [372.49791072] steps:  [2100]\n",
      "Mean reward:  312.76189577538906\n",
      "Max reward:  418.4784216973874\n",
      "It:  2070 reward:  [392.14519598] steps:  [2100]\n",
      "Mean reward:  313.339660734148\n",
      "Max reward:  414.58282543901583\n",
      "It:  2100 reward:  [397.13401214] steps:  [2100]\n",
      "Mean reward:  287.56892950423116\n",
      "Max reward:  417.1387553196576\n",
      "It:  2130 reward:  [395.03750517] steps:  [2100]\n",
      "Mean reward:  262.2300075959332\n",
      "Max reward:  423.22517788155807\n",
      "It:  2160 reward:  [-164.728137] steps:  [2100]\n",
      "Mean reward:  237.87011046634458\n",
      "Max reward:  416.5722330655674\n",
      "It:  2190 reward:  [278.92720891] steps:  [2100]\n",
      "Mean reward:  325.8366810924745\n",
      "Max reward:  420.45289955645933\n",
      "It:  2220 reward:  [384.69901271] steps:  [2100]\n",
      "Mean reward:  307.15362238808103\n",
      "Max reward:  441.9710063704956\n",
      "It:  2250 reward:  [13.92033418] steps:  [2100]\n",
      "Mean reward:  356.666494743398\n",
      "Max reward:  432.335866042143\n",
      "It:  2280 reward:  [295.56962341] steps:  [2100]\n",
      "Mean reward:  278.6733522307779\n",
      "Max reward:  452.5788219380693\n",
      "It:  2310 reward:  [-87.68710261] steps:  [2100]\n",
      "Mean reward:  241.77407941016193\n",
      "Max reward:  449.24854984710254\n",
      "It:  2340 reward:  [294.83974055] steps:  [2100]\n",
      "Mean reward:  227.69560958600124\n",
      "Max reward:  449.71246155849707\n",
      "It:  2370 reward:  [433.32784134] steps:  [2100]\n",
      "Mean reward:  324.49618616881736\n",
      "Max reward:  450.7893130826892\n",
      "It:  2400 reward:  [297.29060945] steps:  [2100]\n",
      "Mean reward:  160.3140702554294\n",
      "Max reward:  444.4257081260278\n",
      "It:  2430 reward:  [409.30189063] steps:  [2100]\n",
      "Mean reward:  290.1066358014007\n",
      "Max reward:  426.85956716551044\n",
      "It:  2460 reward:  [269.28099808] steps:  [2100]\n",
      "Mean reward:  239.18385525977027\n",
      "Max reward:  456.01600096430246\n",
      "It:  2490 reward:  [317.46643955] steps:  [2100]\n",
      "Mean reward:  297.1068586790309\n",
      "Max reward:  439.217300071767\n",
      "It:  2520 reward:  [435.44872416] steps:  [2100]\n",
      "Mean reward:  263.28424672932357\n",
      "Max reward:  435.44872415696756\n",
      "It:  2550 reward:  [292.01649277] steps:  [2100]\n",
      "Mean reward:  318.7993698275846\n",
      "Max reward:  475.5150957303607\n",
      "It:  2580 reward:  [442.23374656] steps:  [2100]\n",
      "Mean reward:  352.69557889927313\n",
      "Max reward:  455.84210082488664\n",
      "It:  2610 reward:  [407.3433342] steps:  [2100]\n",
      "Mean reward:  383.56265037734204\n",
      "Max reward:  454.7420802008418\n",
      "It:  2640 reward:  [119.25046202] steps:  [2100]\n",
      "Mean reward:  346.3243813281741\n",
      "Max reward:  428.12126592970475\n",
      "It:  2670 reward:  [148.96682683] steps:  [2100]\n",
      "Mean reward:  308.6692567665068\n",
      "Max reward:  435.5079699742017\n",
      "It:  2700 reward:  [439.28952762] steps:  [2100]\n",
      "Mean reward:  262.2576975760413\n",
      "Max reward:  449.6179470282492\n",
      "It:  2730 reward:  [425.91555788] steps:  [2100]\n",
      "Mean reward:  231.53913526946246\n",
      "Max reward:  438.9754816721732\n",
      "It:  2760 reward:  [288.60930262] steps:  [2100]\n",
      "Mean reward:  292.3190885040647\n",
      "Max reward:  437.1163679805781\n",
      "It:  2790 reward:  [315.65227719] steps:  [2100]\n",
      "Mean reward:  345.45154304089\n",
      "Max reward:  450.14137393210234\n",
      "It:  2820 reward:  [401.12655001] steps:  [2100]\n",
      "Mean reward:  378.74192906954397\n",
      "Max reward:  457.5537135334746\n",
      "It:  2850 reward:  [425.41216435] steps:  [2100]\n",
      "Mean reward:  362.6269913875935\n",
      "Max reward:  444.15951145844383\n",
      "It:  2880 reward:  [408.90124008] steps:  [2100]\n",
      "Mean reward:  324.1412774448649\n",
      "Max reward:  452.23115320080586\n",
      "It:  2910 reward:  [418.67582255] steps:  [2100]\n",
      "Mean reward:  373.69823039694904\n",
      "Max reward:  455.18327404534534\n",
      "It:  2940 reward:  [435.97283931] steps:  [2100]\n",
      "Mean reward:  397.35771631492423\n",
      "Max reward:  461.40569159441196\n",
      "It:  2970 reward:  [451.92557091] steps:  [2100]\n",
      "Mean reward:  340.0306370570193\n",
      "Max reward:  464.0639618234798\n",
      "It:  3000 reward:  [330.40931063] steps:  [2100]\n",
      "Mean reward:  315.4270846424944\n",
      "Max reward:  489.27756129096065\n",
      "It:  3030 reward:  [455.43728246] steps:  [2100]\n",
      "Mean reward:  363.4484261248391\n",
      "Max reward:  490.68964407796335\n",
      "It:  3060 reward:  [482.9976336] steps:  [2100]\n",
      "Mean reward:  417.5072601878991\n",
      "Max reward:  487.61991074271236\n",
      "It:  3090 reward:  [290.54430965] steps:  [2100]\n",
      "Mean reward:  421.131454025092\n",
      "Max reward:  498.969864170154\n",
      "It:  3120 reward:  [486.58202406] steps:  [2100]\n",
      "Mean reward:  402.3848901154943\n",
      "Max reward:  501.5967201087648\n",
      "It:  3150 reward:  [219.86652918] steps:  [2100]\n",
      "Mean reward:  399.34672683413004\n",
      "Max reward:  505.735160014264\n",
      "It:  3180 reward:  [507.77631103] steps:  [2100]\n",
      "Mean reward:  406.5588708699031\n",
      "Max reward:  526.7918974540144\n",
      "It:  3210 reward:  [358.3838609] steps:  [2100]\n",
      "Mean reward:  297.2835122025612\n",
      "Max reward:  501.5265141687034\n",
      "It:  3240 reward:  [493.10307813] steps:  [2100]\n",
      "Mean reward:  402.5659149256558\n",
      "Max reward:  531.8299770936363\n",
      "It:  3270 reward:  [388.78707175] steps:  [2100]\n",
      "Mean reward:  361.58029337013863\n",
      "Max reward:  540.0005757357243\n",
      "It:  3300 reward:  [500.01765657] steps:  [2100]\n",
      "Mean reward:  331.5135853780449\n",
      "Max reward:  523.3366789114103\n",
      "It:  3330 reward:  [251.93815708] steps:  [2100]\n",
      "Mean reward:  356.1753446563962\n",
      "Max reward:  531.2951803372091\n",
      "It:  3360 reward:  [498.12838261] steps:  [2100]\n",
      "Mean reward:  350.2361603326031\n",
      "Max reward:  511.5587362816581\n",
      "It:  3390 reward:  [503.88076854] steps:  [2100]\n",
      "Mean reward:  325.0897300487477\n",
      "Max reward:  521.836933578225\n",
      "It:  3420 reward:  [365.24825621] steps:  [2100]\n",
      "Mean reward:  360.1312663484121\n",
      "Max reward:  515.8162200086881\n",
      "It:  3450 reward:  [270.20438292] steps:  [2100]\n",
      "Mean reward:  309.17830777026626\n",
      "Max reward:  523.0657169046419\n",
      "It:  3480 reward:  [387.88124536] steps:  [2100]\n",
      "Mean reward:  348.99843438168693\n",
      "Max reward:  556.9446543409867\n",
      "It:  3510 reward:  [490.73581451] steps:  [2100]\n",
      "Mean reward:  323.121750753959\n",
      "Max reward:  535.291329549225\n",
      "It:  3540 reward:  [446.91923906] steps:  [2100]\n",
      "Mean reward:  315.1774122355244\n",
      "Max reward:  536.1198172388306\n",
      "It:  3570 reward:  [157.04203271] steps:  [2100]\n",
      "Mean reward:  394.2263145671868\n",
      "Max reward:  500.92468291527143\n",
      "It:  3600 reward:  [326.78609609] steps:  [2100]\n",
      "Mean reward:  394.57632352166524\n",
      "Max reward:  516.2676332282005\n",
      "It:  3630 reward:  [525.78364674] steps:  [2100]\n",
      "Mean reward:  419.8517787931656\n",
      "Max reward:  525.7836467413543\n",
      "It:  3660 reward:  [436.00791867] steps:  [2100]\n",
      "Mean reward:  367.5213547166772\n",
      "Max reward:  518.1911219411313\n",
      "It:  3690 reward:  [519.4018103] steps:  [2100]\n",
      "Mean reward:  390.8631320607785\n",
      "Max reward:  519.4018103019507\n",
      "It:  3720 reward:  [468.22279651] steps:  [2100]\n",
      "Mean reward:  417.60105549676433\n",
      "Max reward:  509.2167010059358\n",
      "It:  3750 reward:  [484.62684878] steps:  [2100]\n",
      "Mean reward:  427.9983932633577\n",
      "Max reward:  514.563257429304\n",
      "It:  3780 reward:  [493.23574812] steps:  [2100]\n",
      "Mean reward:  371.2022187976261\n",
      "Max reward:  531.9966687281545\n",
      "It:  3810 reward:  [333.1698279] steps:  [2100]\n",
      "Mean reward:  357.6991220673006\n",
      "Max reward:  519.1167221149832\n",
      "It:  3840 reward:  [403.00405215] steps:  [2100]\n",
      "Mean reward:  376.36440825384784\n",
      "Max reward:  503.39783451679796\n",
      "It:  3870 reward:  [314.90244397] steps:  [2100]\n",
      "Mean reward:  367.80583352631845\n",
      "Max reward:  510.6276562612875\n",
      "It:  3900 reward:  [449.43945775] steps:  [2100]\n",
      "Mean reward:  392.9374108576848\n",
      "Max reward:  486.30217078404877\n",
      "It:  3930 reward:  [390.63081179] steps:  [2100]\n",
      "Mean reward:  432.64738402712504\n",
      "Max reward:  507.3351790381092\n",
      "It:  3960 reward:  [506.69904127] steps:  [2100]\n",
      "Mean reward:  380.8860104403348\n",
      "Max reward:  527.5468430765973\n",
      "It:  3990 reward:  [544.75523559] steps:  [2100]\n",
      "Mean reward:  402.5147039791239\n",
      "Max reward:  544.7552355878175\n",
      "It:  4020 reward:  [442.51453327] steps:  [2100]\n",
      "Mean reward:  352.39522287805687\n",
      "Max reward:  530.4024656318193\n",
      "It:  4050 reward:  [244.37999579] steps:  [2100]\n",
      "Mean reward:  344.9104344191844\n",
      "Max reward:  515.1456039222403\n",
      "It:  4080 reward:  [233.31546785] steps:  [2100]\n",
      "Mean reward:  317.5349119596204\n",
      "Max reward:  527.8319878883652\n",
      "It:  4110 reward:  [271.67637351] steps:  [2100]\n",
      "Mean reward:  265.4437155916979\n",
      "Max reward:  523.177278726077\n",
      "It:  4140 reward:  [242.07597292] steps:  [2100]\n",
      "Mean reward:  375.03987460563286\n",
      "Max reward:  517.351445864826\n",
      "It:  4170 reward:  [533.62230214] steps:  [2100]\n",
      "Mean reward:  396.48196600541246\n",
      "Max reward:  533.6223021365347\n",
      "It:  4200 reward:  [479.90803999] steps:  [2100]\n",
      "Mean reward:  350.2116403104368\n",
      "Max reward:  534.1799347794096\n",
      "It:  4230 reward:  [485.24817489] steps:  [2100]\n",
      "Mean reward:  417.8688267511644\n",
      "Max reward:  543.1575223343971\n",
      "It:  4260 reward:  [511.47110111] steps:  [2100]\n",
      "Mean reward:  404.4290010937295\n",
      "Max reward:  529.6430590945558\n",
      "It:  4290 reward:  [494.5822482] steps:  [2100]\n",
      "Mean reward:  432.41707955930855\n",
      "Max reward:  524.6408644806676\n",
      "It:  4320 reward:  [459.20312703] steps:  [2100]\n",
      "Mean reward:  444.37734487374314\n",
      "Max reward:  516.1706580635949\n",
      "It:  4350 reward:  [360.03335109] steps:  [2100]\n",
      "Mean reward:  413.22135916144646\n",
      "Max reward:  507.7433083659992\n",
      "It:  4380 reward:  [351.02585527] steps:  [2100]\n",
      "Mean reward:  394.6855505803076\n",
      "Max reward:  491.0866966735558\n",
      "It:  4410 reward:  [369.84318231] steps:  [2100]\n",
      "Mean reward:  372.528931497994\n",
      "Max reward:  479.97034481246453\n",
      "It:  4440 reward:  [399.33005885] steps:  [2100]\n",
      "Mean reward:  400.20303089902006\n",
      "Max reward:  501.37181055339755\n",
      "It:  4470 reward:  [414.7327933] steps:  [2100]\n",
      "Mean reward:  440.4995039680424\n",
      "Max reward:  476.03450499928385\n",
      "It:  4500 reward:  [336.19159479] steps:  [2100]\n",
      "Mean reward:  422.87322161680964\n",
      "Max reward:  485.40996116988237\n",
      "It:  4530 reward:  [441.58781785] steps:  [2100]\n",
      "Mean reward:  461.4783305456514\n",
      "Max reward:  528.9338922535551\n",
      "It:  4560 reward:  [498.75108931] steps:  [2100]\n",
      "Mean reward:  473.6693501827623\n",
      "Max reward:  507.25899186937124\n",
      "It:  4590 reward:  [517.67896686] steps:  [2100]\n",
      "Mean reward:  468.67142481352147\n",
      "Max reward:  517.6789668582363\n",
      "It:  4620 reward:  [523.10495814] steps:  [2100]\n",
      "Mean reward:  469.0780576428159\n",
      "Max reward:  535.705769334403\n",
      "It:  4650 reward:  [540.2425423] steps:  [2100]\n",
      "Mean reward:  464.5085610375958\n",
      "Max reward:  540.2425423037952\n",
      "It:  4680 reward:  [516.54924843] steps:  [2100]\n",
      "Mean reward:  459.2009944788897\n",
      "Max reward:  523.5894680763895\n",
      "It:  4710 reward:  [527.96767259] steps:  [2100]\n",
      "Mean reward:  449.3401422359296\n",
      "Max reward:  527.9676725937485\n",
      "It:  4740 reward:  [486.47470156] steps:  [2100]\n",
      "Mean reward:  461.48943123930997\n",
      "Max reward:  528.6550451971468\n",
      "It:  4770 reward:  [523.84863929] steps:  [2100]\n",
      "Mean reward:  464.9307937954843\n",
      "Max reward:  523.8486392891283\n",
      "It:  4800 reward:  [373.2582721] steps:  [2100]\n",
      "Mean reward:  489.1422640935946\n",
      "Max reward:  530.0095952844754\n",
      "It:  4830 reward:  [528.96292451] steps:  [2100]\n",
      "Mean reward:  496.85402933343073\n",
      "Max reward:  550.9739328935641\n",
      "It:  4860 reward:  [367.96594431] steps:  [2100]\n",
      "Mean reward:  395.33146098423197\n",
      "Max reward:  551.4702600144651\n",
      "It:  4890 reward:  [500.89828259] steps:  [2100]\n",
      "Mean reward:  453.4340277145508\n",
      "Max reward:  552.0929870335942\n",
      "It:  4920 reward:  [528.8291186] steps:  [2100]\n",
      "Mean reward:  448.83558663299226\n",
      "Max reward:  545.606694820493\n",
      "It:  4950 reward:  [516.00069242] steps:  [2100]\n",
      "Mean reward:  450.935036762652\n",
      "Max reward:  533.6055052922464\n",
      "It:  4980 reward:  [556.7618667] steps:  [2100]\n",
      "Mean reward:  493.1034454957973\n",
      "Max reward:  565.9126908130532\n"
     ]
    }
   ],
   "source": [
    "agent.train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**При таком подходе функция игры возвращает суммарный скор не за одну игру, а за некоторое количество итераций. Для того, чтобы посмотреть на скоры за игру, я написала еще 2 функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, num_episodes):\n",
    "        \n",
    "        #self.save_ep = save_ep\n",
    "        #self.q = time.clock()\n",
    "        self.test_rewards = []\n",
    "        self.test_steps = []\n",
    "        back_rew = len(self.test_rewards)\n",
    "        back_step = len(self.test_steps)\n",
    "        sum_rewards = []\n",
    "        for it in tqdm_notebook(range(num_episodes)):\n",
    "            \n",
    "            r, rewards, steps = play_test_game(agent)\n",
    "        \n",
    "            self.test_rewards += rewards\n",
    "            self.test_steps += steps\n",
    "            sum_rewards += [sum(r)]\n",
    "            if (it+1) % 10 == 0:\n",
    "                print(\"It: \", it+1, \"reward: \", rewards, \"steps: \", steps)\n",
    "                print(\"Sum reward: \", np.mean(np.array(sum_rewards[-10:])))\n",
    "                print(\"Mean reward: \", np.mean(np.array(self.test_rewards[back_rew:])))\n",
    "                print(\"Max reward: \", np.max(np.array(self.test_rewards[back_rew:])))\n",
    "                print(\"Mean steps: \", np.mean(np.array(self.test_steps[back_step:])))\n",
    "                back_rew = len(self.test_rewards)\n",
    "                back_step = len(self.test_steps)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_test_game(self):\n",
    "        t = time.clock()\n",
    "        #print(\"START\")\n",
    "        # Reset env\n",
    "        state = np.float32(self.env.reset())\n",
    "        #rewards = []\n",
    "        self.history.clear()\n",
    "        MAX_STEP = 4000\n",
    "        frames = []\n",
    "        it = 1\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.logprobs = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "        self.actions= []\n",
    "        game = 0\n",
    "        rewards = []\n",
    "        sum_rewards = []\n",
    "        steps = []\n",
    "        while True:\n",
    "            state1, reward, is_terminal = self.play_one_step(state, it)\n",
    "            rewards += [reward]\n",
    "            if is_terminal:\n",
    "                state = env.reset()\n",
    "                steps.append(game)\n",
    "                sum_rewards.append(sum(rewards))\n",
    "                rewards = []\n",
    "                game = 0\n",
    "            #rewards.append(reward)\n",
    "            it +=1\n",
    "            game += 1\n",
    "            state = state1\n",
    "            if it == MAX_STEP:\n",
    "                break\n",
    "     \n",
    "        last_value = 0\n",
    "        if not is_terminal:\n",
    "            obs = Tensor(state1[np.newaxis])\n",
    "            value = self.critic(obs)\n",
    "            last_value = value.data[0][0]\n",
    "        returns = calculate_returns(self.rewards, self.dones, last_value)[:-1]\n",
    "        \n",
    "        \n",
    "        ii = np.arange(len(self.actions)).reshape(-1, 1)\n",
    "        self.observations = np.array(self.observations)\n",
    "        self.actions = np.array(self.actions)\n",
    "        self.logprobs = np.array(self.logprobs).reshape(-1, 1)\n",
    "        self.rewards = np.array(self.rewards).reshape(-1, 1)\n",
    "        self.values = np.array(self.values).reshape(-1, 1)\n",
    "        returns = np.array(returns).reshape(-1, 1)\n",
    "        advantages = returns - self.values\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        return self.rewards, sum_rewards, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765cc2aca488466993262d648b1a2121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  10 reward:  [280.24743125883805, 283.6427875553818, 282.2083802269286] steps:  [1086, 1054, 1076]\n",
      "Sum reward:  993.4522671567717\n",
      "Mean reward:  258.8415203106822\n",
      "Max reward:  286.4212890789678\n",
      "Mean steps:  1047.6451612903227\n",
      "It:  20 reward:  [282.0364129842332, 280.03263427407256, 276.88403394669996] steps:  [1079, 1064, 1150]\n",
      "Sum reward:  976.0654892819854\n",
      "Mean reward:  249.5416343969717\n",
      "Max reward:  285.2001960127187\n",
      "Mean steps:  1015.2424242424242\n",
      "It:  30 reward:  [284.0317188853409, 282.71173157895174, 280.87356460438076] steps:  [1062, 1070, 1085]\n",
      "Sum reward:  881.542826200887\n",
      "Mean reward:  195.68406687171347\n",
      "Max reward:  287.37003667373455\n",
      "Mean steps:  906.4736842105264\n"
     ]
    }
   ],
   "source": [
    "test(agent, 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реворд 200-250!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшим learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer = optim.Adam(list(agent.actor.parameters()) + list(agent.critic.parameters()),lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7e61205ebd4b94a7ea413b76639da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  30 reward:  [418.43166938] steps:  [2100]\n",
      "Mean reward:  496.9649341639776\n",
      "Max reward:  569.0230465972048\n",
      "It:  60 reward:  [565.47033536] steps:  [2100]\n",
      "Mean reward:  517.6634445615043\n",
      "Max reward:  566.8098324862757\n",
      "It:  90 reward:  [560.78574121] steps:  [2100]\n",
      "Mean reward:  473.8682746008208\n",
      "Max reward:  571.0199972195215\n",
      "It:  120 reward:  [437.35090979] steps:  [2100]\n",
      "Mean reward:  468.7064923502734\n",
      "Max reward:  577.6538907133813\n",
      "It:  150 reward:  [533.89286545] steps:  [2100]\n",
      "Mean reward:  471.43487704697634\n",
      "Max reward:  562.1608689342512\n",
      "It:  180 reward:  [525.09702021] steps:  [2100]\n",
      "Mean reward:  497.92370666680347\n",
      "Max reward:  565.3669200398966\n",
      "It:  210 reward:  [389.57678034] steps:  [2100]\n",
      "Mean reward:  495.0662304411102\n",
      "Max reward:  563.5162014722368\n",
      "It:  240 reward:  [553.53618538] steps:  [2100]\n",
      "Mean reward:  479.74919853275264\n",
      "Max reward:  559.5932717955366\n",
      "It:  270 reward:  [517.01128038] steps:  [2100]\n",
      "Mean reward:  493.68667861510863\n",
      "Max reward:  561.7754743471135\n",
      "It:  300 reward:  [526.38579348] steps:  [2100]\n",
      "Mean reward:  515.9625094738232\n",
      "Max reward:  543.4105046198923\n",
      "It:  330 reward:  [533.98431833] steps:  [2100]\n",
      "Mean reward:  487.0014770004469\n",
      "Max reward:  550.6814337087338\n",
      "It:  360 reward:  [542.15690895] steps:  [2100]\n",
      "Mean reward:  499.0839164085067\n",
      "Max reward:  565.5806655595853\n",
      "It:  390 reward:  [530.12628364] steps:  [2100]\n",
      "Mean reward:  518.4123817823059\n",
      "Max reward:  556.8039995340207\n",
      "It:  420 reward:  [539.37893457] steps:  [2100]\n",
      "Mean reward:  517.4520196524677\n",
      "Max reward:  569.3204922535532\n",
      "It:  450 reward:  [564.08721697] steps:  [2100]\n",
      "Mean reward:  504.0790873740345\n",
      "Max reward:  567.4952627092691\n",
      "It:  480 reward:  [540.42517591] steps:  [2100]\n",
      "Mean reward:  505.18890199056904\n",
      "Max reward:  565.4299955115164\n",
      "It:  510 reward:  [273.83225421] steps:  [2100]\n",
      "Mean reward:  491.3717349490874\n",
      "Max reward:  566.5895621465817\n",
      "It:  540 reward:  [407.13174577] steps:  [2100]\n",
      "Mean reward:  473.0794552199743\n",
      "Max reward:  566.2237551675701\n",
      "It:  570 reward:  [554.63583949] steps:  [2100]\n",
      "Mean reward:  465.78149214736516\n",
      "Max reward:  566.1692949538837\n",
      "It:  600 reward:  [210.92817809] steps:  [2100]\n",
      "Mean reward:  449.66309080460644\n",
      "Max reward:  564.1931352225631\n",
      "It:  630 reward:  [501.06542841] steps:  [2100]\n",
      "Mean reward:  483.41857283075376\n",
      "Max reward:  565.1847192786084\n",
      "It:  660 reward:  [538.06247024] steps:  [2100]\n",
      "Mean reward:  471.7937892631341\n",
      "Max reward:  550.3584355713381\n",
      "It:  690 reward:  [275.07167611] steps:  [2100]\n",
      "Mean reward:  494.6226701677397\n",
      "Max reward:  553.9900975859243\n",
      "It:  720 reward:  [521.69570079] steps:  [2100]\n",
      "Mean reward:  503.3991541781306\n",
      "Max reward:  559.61551469124\n",
      "It:  750 reward:  [421.86287866] steps:  [2100]\n",
      "Mean reward:  500.7747672571703\n",
      "Max reward:  563.4480964339149\n",
      "It:  780 reward:  [538.58523928] steps:  [2100]\n",
      "Mean reward:  491.11576227673226\n",
      "Max reward:  558.9288800197706\n",
      "It:  810 reward:  [545.44463022] steps:  [2100]\n",
      "Mean reward:  486.7998026956137\n",
      "Max reward:  562.2260128950135\n",
      "It:  840 reward:  [489.21196008] steps:  [2100]\n",
      "Mean reward:  498.10351957050375\n",
      "Max reward:  561.3858075598141\n",
      "It:  870 reward:  [541.20539395] steps:  [2100]\n",
      "Mean reward:  460.45633798233234\n",
      "Max reward:  565.8985462978746\n",
      "It:  900 reward:  [538.08263185] steps:  [2100]\n",
      "Mean reward:  458.9149767365435\n",
      "Max reward:  555.229612420045\n",
      "It:  930 reward:  [421.83407009] steps:  [2100]\n",
      "Mean reward:  486.61180986729835\n",
      "Max reward:  559.31410000667\n",
      "It:  960 reward:  [548.16354431] steps:  [2100]\n",
      "Mean reward:  506.46495809300666\n",
      "Max reward:  567.6938133773873\n",
      "It:  990 reward:  [560.73515956] steps:  [2100]\n",
      "Mean reward:  507.86274980938447\n",
      "Max reward:  568.0114893920116\n"
     ]
    }
   ],
   "source": [
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf3257f00ea47dfaf5642e6ebad2ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  10 reward:  [278.9206992612655, 283.5884674485009, 283.20438695916107] steps:  [1129, 1046, 1067]\n",
      "Sum reward:  1002.6315391094515\n",
      "Mean reward:  271.74705385988403\n",
      "Max reward:  285.0492349628513\n",
      "Mean steps:  1079.032258064516\n",
      "It:  20 reward:  [277.3641827324988, 280.15853992058754, 283.6247429031536] steps:  [1116, 1110, 1054]\n",
      "Sum reward:  957.2050453199217\n",
      "Mean reward:  239.72411984310577\n",
      "Max reward:  287.3153421497251\n",
      "Mean steps:  1001.3529411764706\n",
      "It:  30 reward:  [278.7384583420781, 280.79691445006756, 277.0257924492659] steps:  [1119, 1110, 1135]\n",
      "Sum reward:  987.2706169073577\n",
      "Mean reward:  257.967324861938\n",
      "Max reward:  286.5430724761506\n",
      "Mean steps:  1044.59375\n"
     ]
    }
   ],
   "source": [
    "test(agent, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer = optim.Adam(list(agent.actor.parameters()) + list(agent.critic.parameters()),lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aaebf6df6be47a2bd1278f7a1a278fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  30 reward:  [509.07191881] steps:  [2100]\n",
      "Mean reward:  517.5569877105959\n",
      "Max reward:  581.825526426599\n",
      "It:  60 reward:  [555.15767937] steps:  [2100]\n",
      "Mean reward:  515.611089918066\n",
      "Max reward:  565.1600328751657\n",
      "It:  90 reward:  [534.49638539] steps:  [2100]\n",
      "Mean reward:  513.8005300940164\n",
      "Max reward:  565.265537898598\n",
      "It:  120 reward:  [519.3126806] steps:  [2100]\n",
      "Mean reward:  497.69290181218423\n",
      "Max reward:  567.4048598562802\n",
      "It:  150 reward:  [398.57690148] steps:  [2100]\n",
      "Mean reward:  500.30087831769936\n",
      "Max reward:  558.4811678410692\n",
      "It:  180 reward:  [543.17867061] steps:  [2100]\n",
      "Mean reward:  504.84762204598496\n",
      "Max reward:  545.82724101588\n",
      "It:  210 reward:  [534.28082296] steps:  [2100]\n",
      "Mean reward:  518.8704995490992\n",
      "Max reward:  569.2184552037107\n",
      "It:  240 reward:  [562.91195246] steps:  [2100]\n",
      "Mean reward:  534.4313828546403\n",
      "Max reward:  574.4136485943036\n",
      "It:  270 reward:  [545.10783088] steps:  [2100]\n",
      "Mean reward:  503.5543800625479\n",
      "Max reward:  576.6369043364156\n",
      "It:  300 reward:  [530.48871563] steps:  [2100]\n",
      "Mean reward:  527.699869219748\n",
      "Max reward:  568.9109198174573\n",
      "It:  330 reward:  [568.88661703] steps:  [2100]\n",
      "Mean reward:  531.2097654533995\n",
      "Max reward:  569.6542083316507\n",
      "It:  360 reward:  [562.11797214] steps:  [2100]\n",
      "Mean reward:  534.7266049436101\n",
      "Max reward:  576.3127115141826\n",
      "It:  390 reward:  [439.33265745] steps:  [2100]\n",
      "Mean reward:  530.3944425858825\n",
      "Max reward:  576.5517104167163\n",
      "It:  420 reward:  [568.19426131] steps:  [2100]\n",
      "Mean reward:  544.8409098644697\n",
      "Max reward:  576.1122872150961\n",
      "It:  450 reward:  [555.72304706] steps:  [2100]\n",
      "Mean reward:  537.7729185379984\n",
      "Max reward:  582.3047581397242\n",
      "It:  480 reward:  [434.13004285] steps:  [2100]\n",
      "Mean reward:  504.0236378575294\n",
      "Max reward:  589.1264174074844\n",
      "It:  510 reward:  [551.13270871] steps:  [2100]\n",
      "Mean reward:  512.529952073349\n",
      "Max reward:  587.1120331723934\n",
      "It:  540 reward:  [419.69795659] steps:  [2100]\n",
      "Mean reward:  524.6946646219983\n",
      "Max reward:  578.2695348676256\n",
      "It:  570 reward:  [559.35812461] steps:  [2100]\n",
      "Mean reward:  537.7973316953626\n",
      "Max reward:  574.2397102579205\n",
      "It:  600 reward:  [559.08596408] steps:  [2100]\n",
      "Mean reward:  501.07934661384326\n",
      "Max reward:  580.543872584612\n",
      "It:  630 reward:  [566.68406967] steps:  [2100]\n",
      "Mean reward:  547.5587241985045\n",
      "Max reward:  587.0041906178141\n",
      "It:  660 reward:  [571.11389279] steps:  [2100]\n",
      "Mean reward:  520.4629056424886\n",
      "Max reward:  580.858621700646\n",
      "It:  690 reward:  [587.62681383] steps:  [2100]\n",
      "Mean reward:  523.4231329293024\n",
      "Max reward:  587.6268138286903\n",
      "It:  720 reward:  [568.78802289] steps:  [2100]\n",
      "Mean reward:  485.3314559222199\n",
      "Max reward:  580.4221848921404\n",
      "It:  750 reward:  [580.46967514] steps:  [2100]\n",
      "Mean reward:  505.631709775116\n",
      "Max reward:  597.2712280690866\n",
      "It:  780 reward:  [569.09536514] steps:  [2100]\n",
      "Mean reward:  532.6857890456457\n",
      "Max reward:  584.9163880626849\n",
      "It:  810 reward:  [556.43023689] steps:  [2100]\n",
      "Mean reward:  532.247201890714\n",
      "Max reward:  576.3009551730341\n",
      "It:  840 reward:  [576.69992622] steps:  [2100]\n",
      "Mean reward:  551.8757504291544\n",
      "Max reward:  596.8559802586377\n",
      "It:  870 reward:  [573.48997463] steps:  [2100]\n",
      "Mean reward:  547.5747783541424\n",
      "Max reward:  592.7495504095745\n",
      "It:  900 reward:  [570.32392096] steps:  [2100]\n",
      "Mean reward:  536.3137391447261\n",
      "Max reward:  574.3771421463099\n",
      "It:  930 reward:  [568.36873654] steps:  [2100]\n",
      "Mean reward:  540.5445885086118\n",
      "Max reward:  585.6532871005733\n",
      "It:  960 reward:  [568.87479488] steps:  [2100]\n",
      "Mean reward:  543.8134555648014\n",
      "Max reward:  573.6361014470783\n",
      "It:  990 reward:  [569.71254405] steps:  [2100]\n",
      "Mean reward:  558.2253767481103\n",
      "Max reward:  576.9404038112331\n"
     ]
    }
   ],
   "source": [
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Протестируем еще раз**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb0b7002a65438e9a69f212fafd4eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  10 reward:  [285.79285718311496, 285.18671332217014, 285.4787015959271] steps:  [1027, 1056, 1034]\n",
      "Sum reward:  1033.4723740450122\n",
      "Mean reward:  257.4712938894956\n",
      "Max reward:  289.2843609913308\n",
      "Mean steps:  992.0909090909091\n",
      "It:  20 reward:  [289.8678382258148, 282.3187649111206, 286.910789411992] steps:  [990, 1087, 1008]\n",
      "Sum reward:  1042.9667751003867\n",
      "Mean reward:  257.79210945258916\n",
      "Max reward:  289.8678382258148\n",
      "Mean steps:  990.1515151515151\n",
      "It:  30 reward:  [284.94322414127174, 281.9810024821265, 284.6772572047411] steps:  [1055, 1070, 1046]\n",
      "Sum reward:  1008.0233299204768\n",
      "Mean reward:  249.93985852174004\n",
      "Max reward:  287.88962546116505\n",
      "Mean steps:  1005.71875\n"
     ]
    }
   ],
   "source": [
    "test(agent, 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer = optim.Adam(list(agent.actor.parameters()) + list(agent.critic.parameters()),lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f801d42f3a482da39cf67101ed5f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  30 reward:  [552.59441632] steps:  [2100]\n",
      "Mean reward:  532.3078290818028\n",
      "Max reward:  594.7081834487027\n",
      "It:  60 reward:  [546.11368707] steps:  [2100]\n",
      "Mean reward:  544.90209027139\n",
      "Max reward:  576.9634278992082\n",
      "It:  90 reward:  [377.51744177] steps:  [2100]\n",
      "Mean reward:  509.3816553291921\n",
      "Max reward:  585.6864958076033\n",
      "It:  120 reward:  [540.80764884] steps:  [2100]\n",
      "Mean reward:  484.5868598258657\n",
      "Max reward:  557.3288273031029\n",
      "It:  150 reward:  [563.76811649] steps:  [2100]\n",
      "Mean reward:  478.73615674178996\n",
      "Max reward:  564.8467233030611\n",
      "It:  180 reward:  [435.46128448] steps:  [2100]\n",
      "Mean reward:  477.81649679097956\n",
      "Max reward:  566.6335103314221\n",
      "It:  210 reward:  [433.10261831] steps:  [2100]\n",
      "Mean reward:  459.5372931087723\n",
      "Max reward:  572.3002658459267\n",
      "It:  240 reward:  [577.33707628] steps:  [2100]\n",
      "Mean reward:  476.0205333775392\n",
      "Max reward:  577.3370762791329\n",
      "It:  270 reward:  [565.45842088] steps:  [2100]\n",
      "Mean reward:  471.70477731514274\n",
      "Max reward:  576.9845703906695\n",
      "It:  300 reward:  [8.76065523] steps:  [2100]\n",
      "Mean reward:  434.6363955782169\n",
      "Max reward:  565.1391031580013\n",
      "It:  330 reward:  [38.04166284] steps:  [2100]\n",
      "Mean reward:  346.33539731293325\n",
      "Max reward:  564.786898941374\n",
      "It:  360 reward:  [551.77198923] steps:  [2100]\n",
      "Mean reward:  454.7215957610048\n",
      "Max reward:  563.895360412684\n",
      "It:  390 reward:  [402.58805014] steps:  [2100]\n",
      "Mean reward:  450.36493819413425\n",
      "Max reward:  565.0950001994945\n",
      "It:  420 reward:  [545.11654558] steps:  [2100]\n",
      "Mean reward:  491.2030421662932\n",
      "Max reward:  567.7066648056835\n",
      "It:  450 reward:  [524.26853902] steps:  [2100]\n",
      "Mean reward:  488.5121256232934\n",
      "Max reward:  566.9372804661269\n",
      "It:  480 reward:  [526.38573664] steps:  [2100]\n",
      "Mean reward:  473.96399422083124\n",
      "Max reward:  548.8813383051486\n",
      "It:  510 reward:  [162.49778377] steps:  [2100]\n",
      "Mean reward:  481.54746404102883\n",
      "Max reward:  548.4260291935641\n",
      "It:  540 reward:  [278.7595655] steps:  [2100]\n",
      "Mean reward:  398.4479015364344\n",
      "Max reward:  558.4988260158605\n",
      "It:  570 reward:  [418.89790002] steps:  [2100]\n",
      "Mean reward:  445.4600548896811\n",
      "Max reward:  544.7289526194215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-c96d0a7ef14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-d8f22e4ffe3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_episodes, save_ep, path)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-d8f22e4ffe3c>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self, train, video)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-d8f22e4ffe3c>\u001b[0m in \u001b[0;36mplay_one_step\u001b[0;34m(self, s, train, it)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#    a = self.get_exploration_action(s[np.newaxis])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a03e86202429>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# N, num_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало хуже, поэтому уменьшим лернинг рейт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer = optim.Adam(list(agent.actor.parameters()) + list(agent.critic.parameters()),lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9625cac2edd34b698fafe1d080b98621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  30 reward:  [535.46574832] steps:  [2100]\n",
      "Mean reward:  489.8792912712109\n",
      "Max reward:  558.465690526012\n",
      "It:  60 reward:  [524.08481884] steps:  [2100]\n",
      "Mean reward:  518.0632873046168\n",
      "Max reward:  556.9133857627019\n",
      "It:  90 reward:  [556.15249662] steps:  [2100]\n",
      "Mean reward:  497.24779627550754\n",
      "Max reward:  557.7082806419148\n",
      "It:  120 reward:  [541.82182775] steps:  [2100]\n",
      "Mean reward:  509.5005906029242\n",
      "Max reward:  562.2901667428414\n",
      "It:  150 reward:  [562.98126631] steps:  [2100]\n",
      "Mean reward:  530.1070566398846\n",
      "Max reward:  563.9382865355836\n",
      "It:  180 reward:  [417.23800667] steps:  [2100]\n",
      "Mean reward:  518.1811151509647\n",
      "Max reward:  570.7204511737541\n",
      "It:  210 reward:  [447.38279544] steps:  [2100]\n",
      "Mean reward:  518.2208152996234\n",
      "Max reward:  578.0220678141256\n",
      "It:  240 reward:  [520.78280343] steps:  [2100]\n",
      "Mean reward:  531.7928385217821\n",
      "Max reward:  567.7914074252327\n",
      "It:  270 reward:  [397.23816501] steps:  [2100]\n",
      "Mean reward:  534.5511190049486\n",
      "Max reward:  568.9552389963771\n",
      "It:  300 reward:  [561.5946204] steps:  [2100]\n",
      "Mean reward:  551.880212428056\n",
      "Max reward:  575.7459970782056\n",
      "It:  330 reward:  [430.32617383] steps:  [2100]\n",
      "Mean reward:  542.0577462312639\n",
      "Max reward:  581.3771953017128\n",
      "It:  360 reward:  [562.21607579] steps:  [2100]\n",
      "Mean reward:  519.5508338458571\n",
      "Max reward:  582.3829067658083\n",
      "It:  390 reward:  [582.37845533] steps:  [2100]\n",
      "Mean reward:  543.7564515706518\n",
      "Max reward:  587.4928610063419\n",
      "It:  420 reward:  [572.47490627] steps:  [2100]\n",
      "Mean reward:  523.0344129167376\n",
      "Max reward:  593.594488027429\n",
      "It:  450 reward:  [532.04645296] steps:  [2100]\n",
      "Mean reward:  547.5654336282349\n",
      "Max reward:  605.3725991599994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-c96d0a7ef14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-d8f22e4ffe3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_episodes, save_ep, path)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-d8f22e4ffe3c>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self, train, video)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-d8f22e4ffe3c>\u001b[0m in \u001b[0;36mplay_one_step\u001b[0;34m(self, s, train, it)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaa0137af944b658e59d35d8fed23a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:  10 reward:  [283.3183324934526, 278.26261527390415, 279.8077729158369] steps:  [992, 1063, 1034]\n",
      "Sum reward:  1008.4706502636029\n",
      "Mean reward:  244.35730670643525\n",
      "Max reward:  285.1455442614303\n",
      "Mean steps:  969.3783783783783\n",
      "It:  20 reward:  [282.527140613124, 280.7914866575695, 281.043911025913] steps:  [986, 1032, 1014]\n",
      "Sum reward:  1028.8829056937884\n",
      "Mean reward:  248.62425345260013\n",
      "Max reward:  283.72031183452384\n",
      "Mean steps:  968.8857142857142\n",
      "It:  30 reward:  [281.04918283496596, 279.29338082414023, 95.09223264834955, 277.32329236835403] steps:  [1019, 1028, 787, 1048]\n",
      "Sum reward:  1045.3750058935073\n",
      "Mean reward:  259.24898116955103\n",
      "Max reward:  283.4929008212321\n",
      "Mean steps:  992.030303030303\n"
     ]
    }
   ],
   "source": [
    "test(agent, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итоговый тестовый реворд примерно 250**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
